\documentclass[11pt, a4paper]{article}
\usepackage[parfill]{parskip}
\usepackage[margin=0.75in]{geometry}
\usepackage{url}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{float}

\lstdefinestyle{snippet}{
    numbers = left,
    basicstyle = \ttfamily\footnotesize,
    frame = single,
    breaklines = true,
}

\setlength\parindent{0pt}

\begin{document}
\title{CS547 Advanced Topics in Computer Science\\
\large{Assignment 03 - Multi-Objective Optimisation: The Next Release Problem}}
\author{Aidan O'Grady - 201218150}
\date{}
\maketitle

\section{Overview}
\label{sec:overview}
The task given revolved around the prioritisation of a set of requirements and
a set of customers, and to compare how to achieve an optimal solution utilising
both multi-objective and single-objective optimisation, using a random search
optimisation as a baseline for comparison, achieved with the Opt4J framework.

Known as the Next Release Problem, a software developer has a list of
requirements to be implemented for the next release, each with a certain cost
and dependencies on other requirements. In addition to these requirements, the
developer has a list of customers who each desire different requirements for
their own needs, with each customer having a weight given to them to indicate
their important to the developer.
% section overview (end)

\section{Problem Representation}
\label{sec:problem_representation}
\subsection{The Input File} % (fold)
\label{sub:the_input_file}
The data for requirements and customers were stored in a file made up of three
segments:

\begin{enumerate}
    \item The number of requirements and their cost.
    \item The dependencies between requirements.
    \item The customers, their weight to the company and their own requirements.
\end{enumerate}

Segment 1 starts by declaring the levels of requirements (ignored in evaluation
as per instructions), and is followed by two lines per level: the number of
requirements at said level in one line, and the proceeding line listing the cost
of each requirement. The position of the cost in this line indicates the ID of
the requirement.

Segment 2 starts by declaring the number of dependencies \emph{d}, with the
proceeding \emph{d} lines being pairs of IDs specifying these dependencies.
Again, instructions specify that these can be ignored.

Finally, Segment 3 again starts by declaring the number of customers \emph{c},
with the next \emph{c} lines listing the weight, number of requirements and then
those requirements.
% subsection the_input_file (end)

\subsection{Storing Data} % (fold)
\label{sub:storing_data}
The problem requires maintaining two collections: the requirements and the
customers. To represent these, two classes \emph{Requirement} and
\emph{Customer} were used to contain the required information after being parsed
from the input file.

The \emph{Requirement} class contained the level, cost and list of the IDs of
requirements it depends on. The \emph{Customer} class contains a list of IDs of
requirements it wants, and the customer's weight.
% subsection storing_data (end)

\subsection{Solution Representation} % (fold)
\label{sub:Solution_representation}
When calculating the score and cost of a given solution, a binary string is used
to represent the solution. If there are \(n\) requirements, then the length of
the string is also \(n\). A `0' indicates the requirement is not present, while
a `1' indicates it is.
% subsection genotype_representation (end)

\subsection{Fitness Functions} % (fold)
\label{sub:fitness_functions}
As part of evaluating a solution, in both a multi-objective and single-objective
optimisation, the two important factors for evaluating had to be defined: cost
and score.

\subsubsection{Cost} % (fold)
\label{ssub:cost}
The cost of a solution set \(x\) is calculated as follows:
\[- \frac{\sum_{i=1}^{n} cost_i \cdot x_i}{budget} \]
Where \(n\) is the total number of requirements, \(cost_i\) is the cost of
requirement \(i\). The budget is the sum of all requirement costs multiplied by
a cost-ratio (typically between 0.3 and 0.5). If the budget is 300, and a
solution's requirements sum to 330, the cost is evaluated as -1.1. The cost is
negated to allow for both fitness functions to be maximised for optimisation,
which made the single-objective easier to approach.

For the purposes of multi-objective optimisation, the cost should be maximised
to achieve an optimal solution.
% subsubsection cost (end)

\subsubsection{Score} % (fold)
\label{ssub:score}
Calculating score is slightly more complex. For every present requirement, all
customers who desire that requirement must be obtained. For each of these
customers, the value of that requirement must be calculated.

The value a customer \(c\) places on a requirement \(r\) is:
\[w * \frac{n - i}{1 + 2 + ... + n}\]
Where \(w\) is the weight of the customer, \(n\) is the number of requirements
of \(c\), and \(i\) is the 0-based index of \(r\) in \(c\)'s list.

For the purposes of multi-objective optimisation, the cost should be maximised
to achieve an optimal solution.
% subsubsection score (end)

\subsubsection{Single Objective} % (fold)
\label{ssub:single_objective}
The single-objective optimisation uses a weighted-sum approach using the fitness
functions defined above. The weight is determined by user input using Opt4J, and
since we have a case where one objective is maximised and other is minimised,
the weighted-sum is tweaked to be:
\[F(x) = w \times score(x) + (1 - w) \times cost(x)\]

Where \(x\) is the solution candidate and \(w\) is the user-defined weight
value.
% subsubsection single_objective (end)
% subsection fitness_functions (end)
% section problem_representation (end)

\section{Implementation}
\label{sec:implementation}
Since Opt4J was used for this assignment, the bulk of code implemented was for
the file parsing and classes storing the parsed data.
% section implementation (end)
\subsection{File Parsing} % (fold)
\label{sub:file_parsing}
The file parsing was relatively straightforward. The parser first read each line
into a list for easier manipulation in the future.

Since the file always declares the number of requirements, dependencies and
customers, I was able to use these to split the initial list into three separate
lists each containing a segment of the file. Opt4J's system meant that I was
was more comfortable just throwing exceptions when a problem with the parsing
occurred, since it ensured that Opt4J did not try to use an invalid file. As
such, \emph{Integer.parseInt(string)} was used liberally throughout the parsing
rather than the use of scanner, since if it failed then Opt4J would not be able
to proceed.

The use of injection allowed for the \emph{NextReleaseProblem} constructor to
inject the location of the input file and the cost ratio for budget calculation
to be used. The object would then be able to store the data read from the file.
This object would then later be injected into the \emph{Creator} and
\emph{Evaluator} classes.
% subsection file_parsing (end)

\subsection{Creator} % (fold)
\label{sub:creator}
The creator was very straightforward to implement. Since the representation
being used was a binary string, Opt4J's \emph{BooleanGenotype} was extremely
suitable for this problem. The injected \emph{NextReleaseProblem} contains the
number of requirements, which is passed into the creator's \emph{init} method to
ensure that the resulting genotype has a random boolean for each requirement.

The genotype is essentially a list of boolean values, making it very easy to
decode and convert into a list of \emph{Requirement}s.
% subsection creator (end)

\subsection{Decoder} % (fold)
\label{sub:decoder}
Similarly to the creator, the decoder is very simple. It merely iterates through
the genotype and converts each boolean into a `0' or '1', creating a binary
string that reflects the potential solution. The decoder does not require any
injection, which again makes it very simple to implement.
% subsection decoder (end)

\subsection{Evaluator} % (fold)
\label{sub:evaluator}
Since single-objective and multi-objective optimisation each handle evaluation
different, a \emph{NRPEvaluator} abstract class was implemented. The evaluators
for score and cost are implemented in this class, with classes for both methods
of optimisation extends this class to use these fitness functions as needed.

injection is again used

\subsubsection{Score} % (fold)
\label{ssub:impl_score}
The score evaluator iterates through the phenotype (decoded string), and for
each character equal to `1', the score for that requirement is found.
\emph{NextReleaseProblem}'s \emph{score(int reqIndex)} method then calculates
the score for that requirement based on the index. Section \ref{ssub:score}
shows the formula used to calculate this individual score, with the final score
being the summation of these scores for each present requirement.
% subsubsection impl_score (end)

\subsubsection{Cost} % (fold)
\label{ssub:impl_cost}
The cost also iterates through the phenotype, adding the cost for each present
requirement. This is when divided by the budget calculated using the cost ratio
and negated. During the implementation, various other methods of calculating
cost were considered, such as using the absolute difference between the cost and
budget, rewarding costs closer to the budget rather than simply rewarding
smaller budgets. I felt that this simpler method of evaluating cost as a
percentage would be more ideal with analysing costs.
% subsubsection impl_cost (end)

\subsubsection{Multi-Objective} % (fold)
\label{ssub:impl_multi_objective}
The multi-objective evaluator includes two different objectives, one for score
and one for score. Both objectives are to be maximised, as a result of the cost
negation.
% subsubsection impl_multi_objective (end)

\subsubsection{Single-Objective} % (fold)
\label{ssub:impl_single_objective}
The single-objective evaluator includes an additional method, \emph(eval()),
that uses the weighted-sum to combine the two fitness functions into a single
fitness value.
% subsubsection impl_single_objective (end)
% subsection evaluator (end)

\subsection{Modules} % (fold)
\label{sub:modules}
Separate modules were implemented for multi-objective and single-objective
optimisation. The problem requires an input file and cost-ratio as parameters
for both problems, while the single-objective module also requires a parameter
for the weight as well. The \emph{Constant} annotation ensures that these values
are able to be injected elsewhere as required, with the constructors that
require these parameters injecting the constants to ensure that they can be
used.
% subsection modules (end)

\section{Comparison}
\label{sec:comparison}

\subsection{Configuration} % (fold)
\label{sub:configuration}
A single file was chosen from the classic dataset and realistic dataset, with
the intention to compare four searches:
\begin{itemize}
    \item Multi-Objective using NSGA-II.
    \item Multi-Objective using ElitismSelector (single objective)
    \item Single-Objective weighted-sum
    \item Random Search
\end{itemize}

With Opt4J, the \emph{EvolutionaryAlgorithm} was used for single and multiple
objective optimisation, with the same default configuration of 1000 generations,
alpha of 100, mu and lambda of 25 and a 0.95 crossover rate. \emph{RandomSearch}
had a batch size of 25, and 1000 iterations. When NSGA-II was used, a tournament
size of 10 was selected. For weighted-sum, the sum was incremented by 0.1 from
0.1 up to 0.9, which the 9 results obtained being plotted after determining the
score and cost of the string.
% subsection configuration (end)

\subsection{Analysis} % (fold)
\label{sub:analysis}
Section \ref{sub:results} shows the results, with Figure \ref{fig:classic}
showing the results from a classic dataset, and Figure \ref{fig:realistic} 
showing the results from a realistic dataset.

As you can see, while the classic data showed no problems with the results,
there are some peculiarities regarding the realistic data. It is clear to see
that in both cases, random search is being out-performed by both single and
multi-objective, and indeed rather convincingly. 

\subsubsection{Classic Dataset} % (fold)
\label{ssub:classic_dataset}
The classic dataset shows that multi-objective and single-objective both perform
much better than random search, forming a Pareto front. When focusing on
solutions near the budget (Y-axis value of -1), we can see that the resulting
score falls between 0.6 and 0.7, which is also obtained by the Single-Objective
when the given weight is around 0.725. Closer examining of the phenotypes that
earned this score shows that around 45-50 requirements are being fulfilled in
this area of the graph. In addition, amongst these requirements, 25
\footnote{2 14 22 23 35 36 38 39 42 45 49 58 60 63 66 72 74 75 76 77 78 80 84 90
92 96. Note that indexing means that these values will be 1 less than values in
file.} requirements were consistently found amongst results, suggesting that
these requirements score highly.

It is interesting to note that with the \emph{ElitismSelector} used with the
Multi-Objective module to perform single-objective optimisation, the results
start to worsen off as the cost goes up, with the scores being no better than
random search when 100\% of the budget is surpassed. The random search's result
that is closest to the budget had a score that is half that of the
Multi-Objective/NGSA-II score with the same cost.
% subsubsection classic_dataset (end)

\subsubsection{Realistic Dataset} % (fold)
\label{ssub:realistic_dataset}
The results obtained from the realistic dataset are concerning, since they show
a monumental difference between the multi-objective NSGA-II results and the
single-objective results, with the latter performing far better. Multi-objective
is still shown to be better than a random search, but the results from it are
far from ideal.

The only results obtained from multi-objective NSGA-II are with scores between
0.5 and 0.65 with costs between -1.25 and -1.6, which is very different from the
results from the classic dataset, this suggests that the optimisation is having
problems with the much larger dataset. The single-objective however, does not
have these problems, creating a similar Pareto front seen with the classic
dataset. It suggests that using 100\% of the budget should yield a score of
around 0.6 to 0.65, but instead multi-objective NSGA-II does not return any
results with near that cost. This means that it is not possible to measure any
meaningful trends in results that hover around the budget like with the classic
dataset.

What is interesting is seeing what happens when the multi-objective module uses
the \emph{ElitismSelector} rather than NSGA-II. It shows that the resultant
single-objective created follows the same pattern it did with the classic
dataset, suggesting that the problem is within NSGA-II's selector instead.

Overall, it seems that further tweaking would be required to improve
multi-objective \& NSGA-II's score for the realistic dataset. Experimentation
with the tournament size did not yield useful information for this purpose. I
was unable to discover the change required to ensure that multi-objective
NSGA-II could create the same type of Pareto front with the realistic dataset
that it did with classic. The much longer runtime for the realistic dataset made
further investigation too time-consuming beyond small changes such as changing
cost-ratio or tournament size, meaning that the source could not be found in
time.
% subsubsection realistic_dataset (end)
% subsection analysis (end)

\newpage
\subsection{Results} % (fold)
\label{sub:results}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{img/classic}
    \caption{Classic dataset results obtained from nrp1.txt}
    \label{fig:classic}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{img/realistic}
    \caption{Realistic dataset results obtained from nrp-e1.txt}
    \label{fig:realistic}
\end{figure}
% subsection results (end)
% section comparison (end)

% \bibliographystyle{plain}
% \bibliography{report} 
\end{document}